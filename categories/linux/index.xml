<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Linux on 不一样的天空</title>
    <link>https://feilengcui008.github.io/categories/linux/</link>
    <description>Recent content in Linux on 不一样的天空</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 21 Sep 2017 16:32:02 +0800</lastBuildDate>
    
	<atom:link href="https://feilengcui008.github.io/categories/linux/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>ABI</title>
      <link>https://feilengcui008.github.io/post/abi/</link>
      <pubDate>Thu, 21 Sep 2017 16:32:02 +0800</pubDate>
      
      <guid>https://feilengcui008.github.io/post/abi/</guid>
      <description>ABI指应用二进制接口，规定了二进制程序两个模块之间或者二进制程序与操作系统之间的接口，这里主要关注调用规范call convention。不同的体系结构、操作系统、编程语言、每种编程语言的不同编译器实现基本都有自己规定或者遵循的ABI和调用规范。另外，也可通过FFI规范实现跨编程语言的过程调用，比如Python/Java/Go等提供了C的FFI，这样通过C实现互相调用。
Linux在x86_64和i386下的ABI:
 x86下的调用规范 Linux i386 and x86_64 call convention x86_64下用户态程序和系统调用ABI i386下用户态和系统调用ABI  这里就不详细解释不同的ABI和调用规范了，可以通过简单的C/C++程序和内核代码分别验证用户态和系统调用的规范。另外，对于类似Go语言有自己的一套函数调用规范的，也可以通过生成的汇编去验证。</description>
    </item>
    
    <item>
      <title>Linux下的时间</title>
      <link>https://feilengcui008.github.io/post/linux%E4%B8%8B%E7%9A%84%E6%97%B6%E9%97%B4/</link>
      <pubDate>Mon, 16 May 2016 17:03:34 +0800</pubDate>
      
      <guid>https://feilengcui008.github.io/post/linux%E4%B8%8B%E7%9A%84%E6%97%B6%E9%97%B4/</guid>
      <description>时钟  硬件时钟
 RTC(real time clock)，记录wall clock time，硬件对应到/dev/rtc设备文件，读取设备文件可得到硬件时间 读取方式  通过ioctl  #include &amp;lt;linux/rtc.h&amp;gt; int ioctl(fd, RTC_request, param);  hwclock命令  通常内核在boot以及从低电量中恢复时，会读取RTC更新system time  软件时钟
 HZ and jiffies, 由内核维护，对于PC通常HZ配置为 1s / 10ms = 100 精度影响select等依赖timeout的系统调用 HRT(high-resolution timers). Linux 2.6.21开始，内核支持高精度定时器，不受内核jiffy限制，可以达到硬件时钟的精度。  外部时钟
 从网络ntp，原子钟等同步   时间  时间类别
 wall clock time =&amp;gt; 硬件时间 real time =&amp;gt; 从某个时间点(比如Epoch)开始的系统时间 sys and user time =&amp;gt; 通常指程序在内核态和用户态花的时间  时间的表示
 time_t 从Epoch开始的秒数 calendar time 字符串 拆分时间 struct tm  struct tm { int tm_sec; /* seconds */ int tm_min; /* minutes */ int tm_hour; /* hours */ int tm_mday; /* day of the month */ int tm_mon; /* month */ int tm_year; /* year */ int tm_wday; /* day of the week */ int tm_yday; /* day in the year */ int tm_isdst; /* daylight saving time */ };   struct timeval/struct timespec  struct timeval { time_t seconds; suseconds_t useconds; } struct timespec { time_t tv_sec; /* seconds */ long tv_nsec; /* nanoseconds */ };   系统时间的操作 #include &amp;lt;time.</description>
    </item>
    
    <item>
      <title>Linux下ELF文件格式</title>
      <link>https://feilengcui008.github.io/post/linux%E4%B8%8Belf%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F/</link>
      <pubDate>Sun, 13 Sep 2015 21:57:01 +0800</pubDate>
      
      <guid>https://feilengcui008.github.io/post/linux%E4%B8%8Belf%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F/</guid>
      <description>In this article,I will talk about the ELF files for Linux,and their diffs as to comprehend the linking process in a diff view angle.There are many articles talking about that topic in the way of compiling-&amp;gt;linking-&amp;gt;loading-&amp;gt;executing.I think once we understand the ELF files,then we can understand why and how and understand the whole process more precisely.
ELF(Executable and Linkable Format) is the default file format of executable files,object files,shared object files and core file for Linux.</description>
    </item>
    
    <item>
      <title>Linux下x86_64进程地址空间布局</title>
      <link>https://feilengcui008.github.io/post/linux%E4%B8%8Bx86_64%E8%BF%9B%E7%A8%8B%E5%9C%B0%E5%9D%80%E7%A9%BA%E9%97%B4%E5%B8%83%E5%B1%80/</link>
      <pubDate>Sun, 08 Mar 2015 23:33:03 +0800</pubDate>
      
      <guid>https://feilengcui008.github.io/post/linux%E4%B8%8Bx86_64%E8%BF%9B%E7%A8%8B%E5%9C%B0%E5%9D%80%E7%A9%BA%E9%97%B4%E5%B8%83%E5%B1%80/</guid>
      <description>关于Linux 32位内存下的内存空间布局，可以参考这篇博文Linux下C程序进程地址空间局关于源代码中各种数据类型/代码在elf格式文件以及进程空间中所处的段，在x86_64下和i386下是类似的，本文主要关注vm.legacy_va_layout以及kernel.randomize_va_space参数影响下的进程空间内存宏观布局，以及vDSO和多线程下的堆和栈分布。
情形一：  vm_legacy_va_layout=1
 kernel.randomize_va_space=0 此种情况下采用传统内存布局方式，不开启随机化 cat 程序的内存布局 可以看出: 代码段：0x400000&amp;ndash;&amp;gt; 数据段 堆：向上增长 2aaaaaaab000&amp;ndash;&amp;gt; 栈：7ffffffde000&amp;lt;&amp;ndash;7ffffffff000 系统调用：ffffffffff600000-ffffffffff601000 你可以试一下其他程序，在kernel.randomize_va_space=0时堆起点是不变的  情形二：  vm_legacy_va_layout=0
 kernel.randomize_va_space=0 现在默认内存布局，不随机化 可以看出: 代码段：0x400000&amp;ndash;&amp;gt; 数据段 堆：向下增长 &amp;lt;&amp;ndash;7ffff7fff000 栈：7ffffffde000&amp;lt;&amp;ndash;7ffffffff000 系统调用：ffffffffff600000-ffffffffff601000  情形三：  vm_legacy_va_layout=0
 kernel.randomize_va_space=2 //ubuntu 14.04默认值 使用现在默认布局，随机化 对比两次启动的cat程序，其内存布局堆的起点是变化的，这从一定程度上防止了缓冲区溢出攻击。  情形四：  vm_legacy_va_layout=1 kernel.randomize_va_space=2 //ubuntu 14.04默认值 与情形三类似，不再赘述  vDSO 在前面谈了两个不同参数下的进程运行时内存空间宏观的分布。也许你会注意到这样一个细节，在每个进程的stack以上的地址中，有一段动态变化的映射地址段，比如下面这个进程，映射到vdso。 &amp;gt; 如果我们用ldd看相应的程序，会发现vdso在磁盘上没有对应的so文件。 不记得曾经在哪里看到大概这样一个问题： &amp;gt; getpid，gettimeofday是不是系统调用？
其实这个问题的答案就和vDSO有关，杂x86_64和i386上，getpid是系统调用，而gettimeofday不是。
vDSO全称是virtual dynamic shared object，是一种内核将一些本身应该是系统调用的直接映射到用户空间，这样对于一些使用比较频繁的系统调用，直接在用户空间调用可以节省开销。如果想详细了解，可以参考这篇文档
下面我们用一段程序验证下：
#include &amp;lt;stdio.h&amp;gt; #include &amp;lt;sys/time.h&amp;gt; #include &amp;lt;sys/syscall.</description>
    </item>
    
    <item>
      <title>Linux网络编程小结</title>
      <link>https://feilengcui008.github.io/post/linux%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%B0%8F%E7%BB%93/</link>
      <pubDate>Wed, 04 Mar 2015 22:37:15 +0800</pubDate>
      
      <guid>https://feilengcui008.github.io/post/linux%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%B0%8F%E7%BB%93/</guid>
      <description>网络编程是一个很大也很有趣的话题，要写好一个高性能并且bug少的服务端或者客户端程序还是挺不容易的，而且往往涉及到进程线程管理、内存管理、协议栈、并发等许多相关的知识，而不仅仅只是会使用socket那么简单。
网络编程模型  阻塞和非阻塞 阻塞和非阻塞通常是指文件描述符本身的属性。对于默认阻塞的socket来说，当socket读缓冲区中没有数据或者写缓冲区满时，都会造成read/recv或者write/send系统调用阻塞，而非阻塞socket在这种情况下会产生EWOULDBLOCK或者EAGAIN等错误并立即返回，不会等待socket变得可读或者可写。在Linux下我们可以通过accept4/fcntl系统调用设置socket为非阻塞。
 同步/异步 同步和异步可以分两层理解。一个是底层OS提供的IO基础设施的同步和异步，另一个是编程方式上的同步和异步。同步IO和异步IO更多地是怎么处理读写问题的一种手段。通常这也对应着两种高性能网络编程模式reactor和proactor。同步通常是事件发生时主动读写数据，直到显示地返回读写状态标志；而异步通常是我们交给操作系统帮我们读写，只需要注册读写完成的回调函数，提交读写的请求后，控制权就返回到进程。对于编程方式上的异步，典型的比如事件循环的回调、C++11的std::async/std::future等等，更多的是通过回调或者线程的方式组织异步的代码逻辑。
 IO复用 IO复用通常是用select/poll/epoll等来统一代理多个socket的事件的发生。select是一种比较通用的多路复用技术，poll是Linux平台下对select做的改进，而epoll是目前Linux下最常用的多路复用技术。
  常见网络库采用的模型(只看epoll)：  nginx：master进程+多个worker进程，one eventloop per process memcached：主线程+多个worker线程，one eventloop per thread tornado：单线程，one eventloop per thread muduo：网络库，one eventloop per thread libevent、libev、boost.asio：网络库，跨平台eventloop封装 &amp;hellip;  排除掉传统的单线程、多进程、多线程等模型，最常用的高性能网络编程模型是one eventloop per thread与多线程的组合。另外，为了处理耗时的任务再加上线程池，为了更好的内存管理再加上对象池。
应用层之外 前面的模型多是针对应用层的C10K类问题的解决方案，在更高并发要求的环境下就需要在内核态下做手脚了，比如使用零拷贝等技术，直接越过内核协议栈，实现高速数据包的传递，相应的内核模块也早有实现。主要的技术点在于：
 数据平面与控制平面分离，减少不必要的系统调用 用户态驱动uio/vfio等减少内存拷贝 使用内存池减少内存分配 通过CPU亲和性提高缓存命中率 网卡多队列与poll模式充分利用多核 batch syscall 用户态协议栈 &amp;hellip;  相应的技术方案大多数是围绕这些点来做优化结合的。比如OSDI &amp;lsquo;14上的Arrakis、IX，再早的有pfring、netmap、intel DPDK、mTCP等等。</description>
    </item>
    
  </channel>
</rss>