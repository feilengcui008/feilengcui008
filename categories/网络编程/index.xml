<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>网络编程 on 不一样的天空</title>
    <link>https://feilengcui008.github.io/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/</link>
    <description>Recent content in 网络编程 on 不一样的天空</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 19 Jul 2017 20:06:51 +0800</lastBuildDate>
    
	<atom:link href="https://feilengcui008.github.io/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Tcplayer介绍</title>
      <link>https://feilengcui008.github.io/post/tcplayer%E4%BB%8B%E7%BB%8D/</link>
      <pubDate>Wed, 19 Jul 2017 20:06:51 +0800</pubDate>
      
      <guid>https://feilengcui008.github.io/post/tcplayer%E4%BB%8B%E7%BB%8D/</guid>
      <description>Tcplayer是最近写的一个流量抓取、解析、放大、重放工具。
背景 通常真实流量相比手工构造的请求来说，更有利于测试。真实流量的回放大体上有以下三种方式：
 应用层  这种方式通常是在服务中耦合拷贝请求的代码。由于直接工作在应用层，截下来的流量是一个个完整的请求，所以其支持场景最多，实现也相对简单，但是需要耦合其他代码，也会给服务程序带来资源消耗。
 网络层  这种方式通常是抓取原始网络包，解析出IP报文，修改报文的目的IP和端口，伪造与测试机的TCP会话，回放到测试机。其优点是不需要处理传输层的TCP包排序，也不需要理解上层应用层的请求格式，但其缺点是配置相对复杂，且难以支持长连接。如果在长连接已经建立的情况下抓包回放，由于测试机并未经过任何SYN-SYN/ACK-ACK的请求建立过程，所以所有请求的TCP PUSH包都会被测试机RST丢掉。通常后端RPC服务都是长连接，所以这是一个比较大的问题。这里有一个工作在这一层的开源工具tcpcopy
 传输层  为了解决外部代码依赖以及长连接的问题，tcplayer基于TCP传输层，按照应用层请求格式解析出请求并重放。这种方式可以抓取到已经建立的长连接的请求，并且服务不需要耦合其他代码，但同时引入了解析和匹配应用层协议的复杂性。
Tcplayer Tcplayer主要包含以下几个步骤:
 libpcap抓取实时流量 tcp包重排序 对于每一个tcp会话(flow)，解析tcp包，尽量匹配应用层协议 放大应用层请求，并与测试服务建立连接回放  目前支持的协议:
 HTTP 1.x 短连接 Thrift strict mode binary protocol GRPC部分支持  这里由于GRPC基于HTTP2，而HTTP2的基本单元Frame没有固定的协议字段，所以无法匹配，导致无法解析到正确的HTTP2请求&amp;hellip;   要给tcplayer添加其他自己定义的应用层协议很容易，可以参考代码factory/thrift</description>
    </item>
    
    <item>
      <title>Linux网络编程小结</title>
      <link>https://feilengcui008.github.io/post/linux%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%B0%8F%E7%BB%93/</link>
      <pubDate>Wed, 04 Mar 2015 22:37:15 +0800</pubDate>
      
      <guid>https://feilengcui008.github.io/post/linux%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%B0%8F%E7%BB%93/</guid>
      <description>网络编程是一个很大也很有趣的话题，要写好一个高性能并且bug少的服务端或者客户端程序还是挺不容易的，而且往往涉及到进程线程管理、内存管理、协议栈、并发等许多相关的知识，而不仅仅只是会使用socket那么简单。
网络编程模型  阻塞和非阻塞 阻塞和非阻塞通常是指文件描述符本身的属性。对于默认阻塞的socket来说，当socket读缓冲区中没有数据或者写缓冲区满时，都会造成read/recv或者write/send系统调用阻塞，而非阻塞socket在这种情况下会产生EWOULDBLOCK或者EAGAIN等错误并立即返回，不会等待socket变得可读或者可写。在Linux下我们可以通过accept4/fcntl系统调用设置socket为非阻塞。
 同步/异步 同步和异步可以分两层理解。一个是底层OS提供的IO基础设施的同步和异步，另一个是编程方式上的同步和异步。同步IO和异步IO更多地是怎么处理读写问题的一种手段。通常这也对应着两种高性能网络编程模式reactor和proactor。同步通常是事件发生时主动读写数据，直到显示地返回读写状态标志；而异步通常是我们交给操作系统帮我们读写，只需要注册读写完成的回调函数，提交读写的请求后，控制权就返回到进程。对于编程方式上的异步，典型的比如事件循环的回调、C++11的std::async/std::future等等，更多的是通过回调或者线程的方式组织异步的代码逻辑。
 IO复用 IO复用通常是用select/poll/epoll等来统一代理多个socket的事件的发生。select是一种比较通用的多路复用技术，poll是Linux平台下对select做的改进，而epoll是目前Linux下最常用的多路复用技术。
  常见网络库采用的模型(只看epoll)：  nginx：master进程+多个worker进程，one eventloop per process memcached：主线程+多个worker线程，one eventloop per thread tornado：单线程，one eventloop per thread muduo：网络库，one eventloop per thread libevent、libev、boost.asio：网络库，跨平台eventloop封装 &amp;hellip;  排除掉传统的单线程、多进程、多线程等模型，最常用的高性能网络编程模型是one eventloop per thread与多线程的组合。另外，为了处理耗时的任务再加上线程池，为了更好的内存管理再加上对象池。
应用层之外 前面的模型多是针对应用层的C10K类问题的解决方案，在更高并发要求的环境下就需要在内核态下做手脚了，比如使用零拷贝等技术，直接越过内核协议栈，实现高速数据包的传递，相应的内核模块也早有实现。主要的技术点在于：
 数据平面与控制平面分离，减少不必要的系统调用 用户态驱动uio/vfio等减少内存拷贝 使用内存池减少内存分配 通过CPU亲和性提高缓存命中率 网卡多队列与poll模式充分利用多核 batch syscall 用户态协议栈 &amp;hellip;  相应的技术方案大多数是围绕这些点来做优化结合的。比如OSDI &amp;lsquo;14上的Arrakis、IX，再早的有pfring、netmap、intel DPDK、mTCP等等。</description>
    </item>
    
  </channel>
</rss>