<!DOCTYPE html>
<html lang="en-us">
	<head>
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="author" content="feilengcui008">
<meta name="generator" content="Hugo 0.22-DEV" />
<title>支持向量机</title>
<link rel="shortcut icon" href="https://feilengcui008.github.io/images/favicon.ico">
<link rel="stylesheet" href="https://feilengcui008.github.io/css/style.css">
<link rel="stylesheet" href="https://feilengcui008.github.io/css/main.css">

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.6.0/styles/default.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.6.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  </head>
  <body>
    <nav class="main-nav">

<div class="sidebar-toggle-parent" onmouseover="mouseOver()" onmouseout="mouseOut()">
  <div class="sidebar-toggle sidebar-toggle-0">
    <a href="/"><span class="hfont">H</span></a>
  </div>
  <div class="sidebar-toggle sidebar-toggle-1">
    <a href="/post"><span class="hfont">A</span></a>
  </div>
  
</div>

</nav>

    <section id="wrapper">
        
        
<div class="content content-post CENTER">
  <article id="支持向量机" class="article article-type-post" itemprop="blogPost">
  <header class="article-header">
    <h1 class="post-title">支持向量机</h1>

    <div class="article-meta">
      <span>
        <i class="icon-calendar"></i>
        <span>05-24-2015</span>
      </span>
    </div>
  </header>

  <div class="article-content">
    

<h3 id="1-概述">1.概述</h3>

<p>SVM全称Support Vector Machine，即支持向量机，是机器学习中的一种监督学习分类算法，一般用于二分类问题。对于线性可分的二分类问题，SVM可以直接求解，对于非线性可分问题，也可以通过核函数将低维映射到高维空间从而转变为线性可分。</p>

<h3 id="2-问题的提出">2.问题的提出</h3>

<ul>
<li><p>考虑一个线性可分的二分类问题</p>

<ul>
<li>m个训练样本$x$是特征向量，$y$是目标变量
&gt;${x^{(i)},y^{(i)}},x^{(i)}\in R^n,y^{(i)}\in {1,-1},i=1,2,\cdots,m$
决策函数：$h_{w,b}(x) = g(w^Tx+b)，g(z)=\begin{cases} 1,if\;\;x&gt;0 \    0,if\;\;x&lt;0 \end{cases}$
<img src="http://img.blog.csdn.net/20150524195045259" alt="这里写图片描述" />
直线代表 $w^Tx+b=0$</li>
</ul></li>

<li><p>首先定义一些符号</p>

<ul>
<li><p>functional margin（函数边界）
&gt; $\hat r=min{\hat r^{(i)}},\;i=1,2,\cdots,m;\;\;\hat r^{(i)}=y^{(i)}*(w^Tx^{(i)}+b)$</p></li>

<li><p>geometrical margin（几何边界）
&gt; $r=min{r^{(i)}},\;i=1,2,\cdots,m;\;\;r^{(i)}=\frac{y^{(i)*(w^Tx^{(i)}+b)}}{|w|}$</p></li>

<li><p>符号解释：
&gt; - 函数边界：由于$y^{(i)}$只能取$1,-1$，所以当$w^T*x^{(i)}+b&gt;&gt;0$时，$y=1$和$y=-1$分别表示点分布在距离超平面$w^Tx+b=0$两边很远的地方,注意如果加倍$w$与$x$，函数边界是会加倍的</p></li>

<li><p>目标：几何边界最大，即
&gt; $max{r}$</p></li>
</ul></li>
</ul>

<h3 id="3-问题的转化">3.问题的转化</h3>

<ul>
<li><p>依次转化：</p>

<ul>
<li>$max{r}$</li>
<li>$max{min{r^{(i)}=\frac{y^{(i)}*(w^Tx^{(i)}+b)}{|w|}};i=1,2,\cdots,m}$</li>
<li>$\begin{cases}max{r}\s.t. \;\;\frac{y^{(i)}*(w^Tx^{(i)}+b)}{|w|}\ge r\end{cases}$</li>
<li>$\begin{cases}max{\frac{\hat r}{|w|}}\s.t. \;\;y^{(i)}*(w^Tx^{(i)}+b)\ge \hat r\end{cases}$</li>
<li>注意函数边界的改变不影响优化问题的求解结果
<br /></li>
</ul>

<blockquote>
<p>$let \;\hat r=1$
问题转化为:
$\begin{cases}max{\frac{1}{|w|}}\s.t. \;\;y^{(i)}<em>(w^Tx^{(i)}+b)\ge 1\end{cases}$
最终转化为optimization problem，而且目标函数是convex的，即凸函数
$\begin{cases}min{\frac{1}{2}w^2}\s.t. \;\;y^{(i)}</em>(w^Tx^{(i)}+b)\ge 1\end{cases}最终得到优化问题(1)$</p>
</blockquote></li>
</ul>

<h3 id="4-问题求解">4.问题求解</h3>

<p>(1)可以用通常的QP（二次规划）方法求解，matlab或lingo都有相应工具箱。
(2)既然本文叫SVM，当然会用到不同的解法，而且SVM的解法在训练集很大的时候，比一般的QP解法效率高。</p>

<ul>
<li><p>广义拉格朗日数乘法
&gt;对于3中得到的优化问题（1）有：
$\begin{cases}L(w,b,\alpha)=\frac{1}{2}w^2-\sum_{i=1}^m\alpha^{(i)}[y^{(i)}*(w^Tx^{(i)}+b)-1]\\alpha^{(i)}\ge 0\end{cases}$</p>

<ul>
<li>满足约束条件$y^{(i)}*(w^Tx^{(i)}+b)\ge 1$下有：
$max{L(w,b,\alpha)}=\frac{1}{w^2}=f(w)$</li>
</ul></li>

<li><p>优化问题变为:
&gt; $\begin{cases}min<em>{w,b}{max</em>\alpha{L(w,b,\alpha)}}\s.t. \;\;y^{(i)}*(w^Tx^{(i)}+b)\ge 1 \\alpha^{(i)}\ge 0 \end{cases}$</p></li>

<li><p>在满足KKT条件下有(对偶优化问题)</p>

<ul>
<li>$min<em>{w,b}{max</em>\alpha{L(w,b,\alpha)}}=max<em>{\alpha}{min</em>{w,b}{L(w,b,\alpha)}}$
&gt;通常对偶问题(dual problem)$max{min{f(w,\alpha)}}$比原始问题(primal problem)$min{max{f(w,\alpha)}}$更容易求解，尤其是在训练样本数量很大的情况下,KKT条件又称为互补松弛条件</li>

<li><p>$\nabla_{w,b}L(\bar w,\bar b,\bar \alpha)=0;$
&gt;$\bar w,\bar b是primal\;optimal;\;\;\bar \alpha是dual\;optimal$</p></li>

<li><p>$\bar \alpha^{(i)}g_i(\bar w,\bar b)=0$，
&gt;$y^{(i)}<em>(w^Tx^{(i)}+b)=1$时，通常有$\alpha\ne 0$，这些点称为Support Vector，即支持向量     $y^{(i)}</em>(w^Tx^{(i)}+b)&gt;1$时,有$\alpha=0$，通常大多数$\alpha$为0，减少了计算量</p></li>
</ul></li>

<li><p>解决$min<em>{w,b}{L(w,b,\alpha)}$
&gt;求偏导令为0可得$\begin{cases}w=\sum</em>{i=1}^m\alpha^{(i)}y^{(i)}x^{(i)}\\sum_{i=1}^m\alpha^{(i)}y^{(i)}=0 \end{cases}$</p></li>

<li><p>带入原式:
&gt;$\begin{cases}max<em>\alpha{\sum</em>{i=1}^m\alpha^{(i)}-\frac{1}{2}\sum<em>{i,j=1}^my^{(i)}y^{(j)}\alpha^{(i)   }\alpha^{(j)}<x^{(i)},x^{(j)}>} \ \alpha^{(i)}\ge 0 \      \sum</em>{i=1}^m\alpha^{(i)}y^{(i)}=0\end{cases}$
&gt;- 求得$\alpha$则可得到$w,b$
&gt;- 目标表示为 $w^Tx+b=\sum_{i=1}^m\alpha^{(i)}y^{(i)}<x^{(i)},x>+b$
&gt;- $kernel(x，y)=<x^T,y>$称为核函数，能较少高维空间计算量，通常知道了核函数，计算量相对于找对应的$x,y$向量小得多,而且若$x,y$是无限维向量，也可通过核函数映射。常用的核函数有：</p>

<ul>
<li>高斯核$K(x,z)=exp(-\frac{|z-x|}{2\sigma^2})$</li>
<li>多项式核$K(x,z)=(x-z)^a$</li>
</ul></li>
</ul>

<h3 id="5-问题的优化">5.问题的优化</h3>

<ul>
<li>4中推导出了求$\alpha$使得最大化的问题。但其存在一定问题。
<img src="http://img.blog.csdn.net/20150524195822371" alt="这里写图片描述" /></li>
</ul>

<p>当训练集如右图分布在超平面两侧时，结果并不好，因此我们可以给$\hat r=1$添加松弛条件，允许少数点小于1，甚至分类到错误的一面
+ 我们修改限制条件，并修改目标函数
  &gt; $\begin{cases}min{\frac{1}{2}w^2+csum<em>{i=1}^m\xi</em>{i}}\y^{(i)}*(w^Tx^{(i)}+b)\ge 1-\xi<em>{i} \ \xi</em>{i}\ge 0\end{cases}$</p>

<ul>
<li>通过类似的对偶问题的求解，我们得到
&gt;$\begin{cases}W=max<em>\alpha{\sum</em>{i=1}^m\alpha^{(i)}-\frac{1}{2}\sum<em>{i,j=1}^my^{(i)}y^{(j)}\alpha^{(i)   }\alpha^{(j)}<x^{(i)},x^{(j)}>} \ 0\le \alpha\le c \      \sum</em>{i=1}^m\alpha^{(i)}y^{(i)}=0\end{cases}$</li>
</ul>

<h3 id="6-优化后问题的求解">6.优化后问题的求解</h3>

<ul>
<li><p>坐标上升法求解最大值</p>

<pre><code>#伪代码
    loop {
      for i in range(m):
          alpha(i):=alpha(i) which let {w} maximum
  }
</code></pre></li>

<li><p>坐标上升与梯度上升的对比图
<img src="http://img.blog.csdn.net/20150524200008560" alt="这里写图片描述" /></p></li>

<li><p>SMO</p></li>
</ul>

<pre><code> #伪代码
      L&lt;=alpha&lt;=H
      loop {
          for i,j in range(m):
              alpha(i):=min{ (alpha(i) or L or H ) which let {w} maximum }
              alpha(j):=min{ (alpha(j) or L or H ) which let {w} maximum }
      }
</code></pre>

<h3 id="7-实战">7.实战</h3>

<ul>
<li>trainsets  总共90组<br />
&gt;-0.017612 14.0530640<br />
-1.395634 4.6625411
-0.752157 6.5386200
-1.322371 7.1528530
&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;
-1.076637 -3.1818881
1.821096 10.2839900
3.010150 8.4017661
-1.099458 1.6882741
-0.834872 -1.7338691
-0.846637 3.8490751
1.400102 12.6287810
1.752842 5.4681661
0.078557 0.0597361</li>

<li><p>testsets  总共10组
&gt;0.089392 -0.7153001
1.825662 12.6938080
0.197445 9.7446380
0.126117 0.9223111
-0.679797 1.2205301
0.677983 2.5566661
0.761349 10.6938620
-2.168791 0.1436321
1.388610 9.3419970
0.317029 14.7390250</p></li>

<li><p>logistic回归效果</p>

<ul>
<li>权值$weight=[[ 11.93391219][  1.12324688][ -1.60965531]]$</li>
<li>原始测试文件真值$y=[1.0, 0.0, 0.0, 1.0, 1.0,1.0, 0.0, 1.0, 0.0, 0.0]$</li>
<li>logistic回归预测值:$y1=[1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]$</li>
<li>正确率还是蛮高的</li>
<li>附上代码:</li>
</ul></li>
</ul>

<pre><code>#!/usr/bin/env
#coding:utf-8
import numpy
import sys
from matplotlib import pyplot
import random

def makedata(filename):
    try:
        f = open(filename,&quot;r&quot;)
        lines = f.readlines()
        datalist = []
        datalist = [i.split() for i in lines ]
        datalist = [ [ float(i) for i in line] for line in datalist ]
        for i in range(len(datalist)):
            datalist[i].insert(0,1.0)
    except:
        return
    finally:
        return datalist
        f.close()

def makedat(filename):
    try:
        f = open(filename,&quot;r&quot;)
        lines = f.readlines()
        datalist = []
        datalist = [i.split() for i in lines ]
        datalist = [ [ float(i) for i in line] for line in datalist ]
        x = [ line[0:len(line)-1] for line in datalist ]
        y = [ line[-1] for line in datalist ]
    except:
        return
    finally:
        return x,y
        f.close()

def sigma(z):
    return 1.0/(1+numpy.exp(-z))

#batch regression
def logisticFunc(dataset,itertimes,alpha):
    weight = numpy.ones((len(dataset[0])-1,1))   
    value = [ int(i[-1]) for i in dataset ]
    value = numpy.mat(value).transpose()
    params = [ i[0:-1] for i in dataset ]
    params = numpy.mat(params)
    for i in range(int(itertimes)):
        error = value-sigma(params*weight)
        weight = weight+alpha*params.transpose()*error
    return weight

#random grad ascend regression 
def randLogisticFunc(dataset,itertimes,alpha):
    weight = numpy.ones((len(dataset[0])-1,1))   
    value = [ int(i[-1]) for i in dataset ]
    value = numpy.mat(value).transpose()
    params = [ i[0:-1] for i in dataset ]
    params = numpy.mat(params)
    for i in range(int(itertimes)):
        randid = random.randint(0,len(dataset)-1)
        error = value[randid]-sigma(params[randid]*weight)
        weight = weight+alpha*params[randid].transpose()*error
    return weight


def plot(data,weight):
    x1 = []
    x2 = []
    y1 = []
    y2 = []
    for i in data:
        if i[-1] == 1:
            x1.append(i[1])
            y1.append(i[2])
        else:
            x2.append(i[1])
            y2.append(i[2])
    x = numpy.linspace(-3,3,1000)
    weight = numpy.array(weight)
    y = (-weight[0][0]-weight[1][0]*x)/weight[2][0]
    fg = pyplot.figure()
    sp = fg.add_subplot(111)
    sp.scatter(x1,y1,s=30,c=&quot;red&quot;)
    sp.scatter(x2,y2,s=30,c=&quot;blue&quot;)
    sp.plot(x,y)
    pyplot.show()

def predict(weight,x1):
    yi = []
    for i in x1:
        if weight[0][0]+i[0]*weight[1][0]+i[1]*weight[2][0]&gt;=0:
            yi.append(1)
        else:
            yi.append(0)
    print yi
    

def main():
    trainfile = sys.argv[1]
    itertimes = int(sys.argv[2])
    alpha = float(sys.argv[3])
    testfile = sys.argv[4]
    data = makedata(trainfile)
    testx,testy = makedat(testfile)
    weight = logisticFunc(data,itertimes,alpha)
    print weight
    predict(weight,testx)
    print testy
    #weight = randLogisticFunc(data,itertimes,alpha)
    #print weight
    plot(data,weight)
if __name__=='__main__':
    main()
</code></pre>

<ul>
<li>SVM效果（采用高斯核,使用sklearn库）

<ul>
<li>原始测试文件真值$y=[1.0, 0.0, 0.0, 1.0, 1.0,1.0, 0.0, 1.0, 0.0, 0.0]$</li>
<li>svm预测值:$y1=array([ 1.,  0.,  0.,  1.,  1.,  1.,  0.,  1.,  0.,  0.])$</li>
<li>正确率也挺高的</li>
<li>附上代码:
<br /></li>
</ul></li>
</ul>

<pre><code>#!/usr/bin/env python
#coding:utf-8
from sklearn import svm
import sys
def makedata(filename):
    try:
        f = open(filename,&quot;r&quot;)
        lines = f.readlines()
        datalist = []
        datalist = [i.split() for i in lines ]
        datalist = [ [ float(i) for i in line] for line in datalist ]
        x = [ line[0:len(line)-1] for line in datalist ]
        y = [ line[-1] for line in datalist ]
    except:
        return
    finally:
        return x,y
        f.close()
def learn(x,y):
  clf = svm.SVC()
  clf.fit(x,y)
  return clf
def predict(x1,y1,clf):
  print &quot;svm fit results&quot;,clf.predict(x1)
  print &quot;original test file results&quot;,y1
if __name__==&quot;__main__&quot;:
  inputfile = sys.argv[1]
  testfile = sys.argv[2]
  x,y = makedata(inputfile)
  x1,y1 = makedata(testfile)
  clf = learn(x,y)
  predict(x1, y1, clf)
</code></pre>

  </div>
</article>

</div>
<div class="fexo-comments comments-post">
  <div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'feilengcui008';
    var disqus_identifier = 'https:\/\/feilengcui008.github.io\/post\/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA\/';
    var disqus_title = '支持向量机';
    var disqus_url = 'https:\/\/feilengcui008.github.io\/post\/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA\/';

    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div> 



        <footer id="footer">
</footer>

    </section>
    <script src="https://feilengcui008.github.io/js/app.js"></script>
<script src="https://feilengcui008.github.io/js/head.js"></script>
<script src="https://feilengcui008.github.io/js/bundle.js"></script>
<script src="https://feilengcui008.github.io/js/fastclick.js"></script>
<script src="https://feilengcui008.github.io/js/scroll-spy.js"></script>
<script src="https://feilengcui008.github.io/js/util.js"></script>
<script src="https://feilengcui008.github.io/js/zenscroll.js"></script>



  </body>
</html>
